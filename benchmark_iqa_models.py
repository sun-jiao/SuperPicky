#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IQA Model Benchmark - å¯¹æ¯” NIMA vs MUSIQ-AVA vs NIMA-KonIQ æ€§èƒ½
æµ‹è¯•æ¨ç†æ—¶é—´å’Œè¯„åˆ†å·®å¼‚
"""

import time
import os
import pyiqa
import torch
from PIL import Image
import glob
import traceback


def resize_for_model(img_path: str, max_size: int = 1024) -> torch.Tensor:
    """
    ç¼©æ”¾å›¾ç‰‡ä»¥é€‚åº”æ¨¡å‹å†…å­˜é™åˆ¶
    MUSIQ çš„ Attention æœºåˆ¶å¯¹å¤§å›¾éå¸¸æ¶ˆè€—å†…å­˜
    """
    img = Image.open(img_path).convert('RGB')
    w, h = img.size
    
    # å¦‚æœå›¾ç‰‡å¤ªå¤§ï¼Œç¼©æ”¾åˆ° max_size
    if max(w, h) > max_size:
        scale = max_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    # è½¬æ¢ä¸º tensor
    import torchvision.transforms as T
    transform = T.ToTensor()
    return transform(img).unsqueeze(0)


def test_model(model_name: str, model, test_files: list, device: str, use_resize: bool = False, max_size: int = 1024):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    scores = []
    times = []
    
    for i, img_path in enumerate(test_files, 1):
        try:
            start = time.time()
            
            if use_resize:
                # å¯¹äº MUSIQï¼Œéœ€è¦ç¼©æ”¾å›¾ç‰‡
                img_tensor = resize_for_model(img_path, max_size).to(device)
                score = model(img_tensor).item()
            else:
                score = model(img_path).item()
            
            elapsed = time.time() - start
            scores.append(score)
            times.append(elapsed)
            print(f"  [{i}/{len(test_files)}] {os.path.basename(img_path)}: {score:.3f} ({elapsed*1000:.0f}ms)")
        except Exception as e:
            print(f"  [{i}/{len(test_files)}] {os.path.basename(img_path)}: âŒ é”™è¯¯ - {str(e)[:50]}")
            scores.append(None)
            times.append(None)
    
    valid_times = [t for t in times if t is not None]
    valid_scores = [s for s in scores if s is not None]
    
    avg_time = sum(valid_times) / len(valid_times) if valid_times else 0
    avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0
    
    return {
        'scores': scores,
        'times': times,
        'avg_time': avg_time,
        'avg_score': avg_score,
        'success_rate': len(valid_scores) / len(test_files)
    }


def benchmark_models(image_dir: str, num_images: int = 10):
    """
    å¯¹æ¯”æµ‹è¯• NIMA, MUSIQ-AVA å’Œ NIMA-KonIQ æ¨¡å‹
    """
    device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ”¶é›†æµ‹è¯•å›¾ç‰‡
    jpg_files = glob.glob(os.path.join(image_dir, "*.jpg")) + \
                glob.glob(os.path.join(image_dir, "*.jpeg")) + \
                glob.glob(os.path.join(image_dir, "*.JPG"))
    
    if not jpg_files:
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥ç›®å½•: {image_dir}")
        return
    
    test_files = jpg_files[:num_images]
    print(f"ğŸ“ æµ‹è¯•å›¾ç‰‡: {len(test_files)} å¼ ")
    
    for f in test_files:
        img = Image.open(f)
        print(f"   - {os.path.basename(f)}: {img.size[0]}x{img.size[1]}")
    print()
    
    results = {}
    
    # ==========================================
    # æµ‹è¯• 1: NIMA (å½“å‰ä½¿ç”¨ - InceptionV2 backbone)
    # ==========================================
    print("=" * 60)
    print("ğŸ“Š æ¨¡å‹ 1: NIMA (InceptionV2 backbone) - ä½ ç›®å‰ä½¿ç”¨çš„")
    print("=" * 60)
    
    load_start = time.time()
    nima_model = pyiqa.create_metric('nima', device=device)
    nima_load_time = time.time() - load_start
    print(f"â±ï¸  æ¨¡å‹åŠ è½½æ—¶é—´: {nima_load_time:.2f}s")
    
    results['nima'] = test_model('nima', nima_model, test_files, device)
    results['nima']['load_time'] = nima_load_time
    
    print(f"\nğŸ¯ NIMA å¹³å‡æ¨ç†æ—¶é—´: {results['nima']['avg_time']*1000:.0f}ms")
    print(f"ğŸ¯ NIMA å¹³å‡åˆ†æ•°: {results['nima']['avg_score']:.3f}")
    
    del nima_model
    if device == 'mps':
        torch.mps.empty_cache()
    
    # ==========================================
    # æµ‹è¯• 2: NIMA-KonIQ (æŠ€æœ¯è´¨é‡è¯„ä¼°)
    # ==========================================
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¨¡å‹ 2: NIMA-KonIQ (æŠ€æœ¯è´¨é‡è¯„ä¼° - InceptionV2)")
    print("=" * 60)
    
    load_start = time.time()
    koniq_model = pyiqa.create_metric('nima-koniq', device=device)
    koniq_load_time = time.time() - load_start
    print(f"â±ï¸  æ¨¡å‹åŠ è½½æ—¶é—´: {koniq_load_time:.2f}s")
    
    results['nima-koniq'] = test_model('nima-koniq', koniq_model, test_files, device)
    results['nima-koniq']['load_time'] = koniq_load_time
    
    print(f"\nğŸ¯ NIMA-KonIQ å¹³å‡æ¨ç†æ—¶é—´: {results['nima-koniq']['avg_time']*1000:.0f}ms")
    print(f"ğŸ¯ NIMA-KonIQ å¹³å‡åˆ†æ•°: {results['nima-koniq']['avg_score']:.3f}")
    
    del koniq_model
    if device == 'mps':
        torch.mps.empty_cache()
    
    # ==========================================
    # æµ‹è¯• 3: MUSIQ-AVA (å¤šå°ºåº¦ Transformer) - éœ€è¦ç¼©æ”¾
    # ==========================================
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¨¡å‹ 3: MUSIQ-AVA (Multi-scale Transformer)")
    print("âš ï¸  æ³¨æ„: å¯¹é«˜åˆ†è¾¨ç‡å›¾ç‰‡éœ€è¦ç¼©æ”¾ (max 1024px)")
    print("=" * 60)
    
    load_start = time.time()
    musiq_model = pyiqa.create_metric('musiq-ava', device=device)
    musiq_load_time = time.time() - load_start
    print(f"â±ï¸  æ¨¡å‹åŠ è½½æ—¶é—´: {musiq_load_time:.2f}s")
    
    results['musiq-ava'] = test_model('musiq-ava', musiq_model, test_files, device, use_resize=True, max_size=1024)
    results['musiq-ava']['load_time'] = musiq_load_time
    
    print(f"\nğŸ¯ MUSIQ-AVA å¹³å‡æ¨ç†æ—¶é—´: {results['musiq-ava']['avg_time']*1000:.0f}ms")
    print(f"ğŸ¯ MUSIQ-AVA å¹³å‡åˆ†æ•°: {results['musiq-ava']['avg_score']:.3f}")
    
    del musiq_model
    if device == 'mps':
        torch.mps.empty_cache()
    
    # ==========================================
    # å¯¹æ¯”æ€»ç»“
    # ==========================================
    print("\n" + "=" * 60)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    
    nima_time = results['nima']['avg_time'] * 1000
    koniq_time = results['nima-koniq']['avg_time'] * 1000
    musiq_time = results['musiq-ava']['avg_time'] * 1000
    
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡              â”‚ NIMA (AVA)  â”‚ NIMA-KonIQ  â”‚ MUSIQ-AVA   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹åŠ è½½æ—¶é—´      â”‚ {results['nima']['load_time']:>8.2f}s   â”‚ {results['nima-koniq']['load_time']:>8.2f}s   â”‚ {results['musiq-ava']['load_time']:>8.2f}s   â”‚
â”‚ å¹³å‡æ¨ç†æ—¶é—´      â”‚ {nima_time:>8.0f}ms  â”‚ {koniq_time:>8.0f}ms  â”‚ {musiq_time:>8.0f}ms  â”‚
â”‚ å¹³å‡åˆ†æ•°          â”‚ {results['nima']['avg_score']:>8.3f}    â”‚ {results['nima-koniq']['avg_score']:>8.3f}    â”‚ {results['musiq-ava']['avg_score']:>8.3f}    â”‚
â”‚ æˆåŠŸç‡            â”‚ {results['nima']['success_rate']*100:>7.0f}%   â”‚ {results['nima-koniq']['success_rate']*100:>7.0f}%   â”‚ {results['musiq-ava']['success_rate']*100:>7.0f}%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¯„ä¼°ç»´åº¦          â”‚ ç¾å­¦è´¨é‡    â”‚ æŠ€æœ¯å¤±çœŸ    â”‚ ç¾å­¦è´¨é‡    â”‚
â”‚ æ¶æ„              â”‚ CNN/Incep   â”‚ CNN/Incep   â”‚ Transformer â”‚
â”‚ è¾“å…¥éœ€æ±‚          â”‚ åŸå›¾        â”‚ åŸå›¾        â”‚ éœ€ç¼©æ”¾      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # å»ºè®®
    print("ğŸ’¡ å»ºè®®:")
    if musiq_time > nima_time * 2:
        print(f"  âš ï¸  MUSIQ-AVA æ¯” NIMA æ…¢ {musiq_time/nima_time:.1f}xï¼Œéœ€è¦ç¼©æ”¾é«˜åˆ†è¾¨ç‡å›¾ç‰‡")
        print(f"  âœ… å»ºè®®ä¿æŒ NIMA (AVA) ä½œä¸ºç¾å­¦è¯„ä¼°")
    else:
        print(f"  âœ… MUSIQ-AVA æ€§èƒ½å¯æ¥å—ï¼Œå¯è€ƒè™‘åˆ‡æ¢")
    
    if koniq_time < nima_time * 1.5:
        print(f"  ğŸ’¡ NIMA-KonIQ å¯æ›¿ä»£ BRISQUE è¯„ä¼°æŠ€æœ¯å¤±çœŸ (åŒæ¶æ„ï¼Œæ›´å‡†ç¡®)")
    
    return results


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python benchmark_iqa_models.py <å›¾ç‰‡ç›®å½•> [æµ‹è¯•æ•°é‡]")
        print("ç¤ºä¾‹: python benchmark_iqa_models.py /path/to/bird/photos 10")
        sys.exit(1)
    
    image_dir = sys.argv[1]
    num_images = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    benchmark_models(image_dir, num_images)
